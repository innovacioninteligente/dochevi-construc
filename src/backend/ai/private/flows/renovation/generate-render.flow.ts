import { genkit, z } from 'genkit';
import { ai } from '@/backend/ai/core/config/genkit.config';
import { getGeminiClient } from '@/backend/ai/core/infrastructure/gemini-client';

const RenderInputSchema = z.object({
    imageBuffers: z.array(z.string()).describe("Base64 encoded images"),
    style: z.string(),
    roomType: z.string(),
    additionalRequirements: z.string().optional()
});

const RenderOutputSchema = z.object({
    generatedImage: z.string().describe("Base64 encoded generated image"),
});

export const generateRenderFlow = ai.defineFlow(
    {
        name: 'generateRenderFlow',
        inputSchema: RenderInputSchema,
        outputSchema: RenderOutputSchema,
    },
    async (input) => {
        const client = getGeminiClient();

        // 1. Render the text prompt using the .prompt file architecture
        // We use the Genkit prompt reference to separate prompt logic from code.
        const renovationPrompt = ai.prompt('renovation-render');

        // We render the prompt template to text
        const rendered = await renovationPrompt.render({
            style: input.style,
            roomType: input.roomType,
            additionalRequirements: input.additionalRequirements,
        });

        // Extract the text content from the rendered prompt
        // Use optional chaining for safety as 'messages' might be undefined in some Genkit versions/contexts
        const promptText = rendered.messages?.[0]?.content.find(c => c.text)?.text || '';

        if (!promptText) {
            throw new Error("Failed to render prompt text from renovation-render.prompt: No text content generated.");
        }

        // 2. Prepare the payload for Gemini 2.5 Flash Image (Raw SDK)
        const imageParts = input.imageBuffers.map(buffer => ({
            inlineData: {
                data: buffer,
                mimeType: "image/jpeg"
            }
        }));

        // 3. Call the model
        // Note: We use the raw SDK because Genkit's high-level abstractions for "Flash Image" 
        // specifically with inline Base64 might differ from the standard Vertex/GoogleAI plugin expectations.
        const response = await client.models.generateContent({
            model: "gemini-2.5-flash", // Reverting to flash as flash-image is deprecated/alias in some SDKs for multi-modal
            contents: [
                { role: 'user', parts: [{ text: promptText }, ...imageParts] }
            ],
            config: {
                responseModalities: ['IMAGE'],
                // imageConfig: {
                //    aspectRatio: "16:9" 
                // } 
                // Let model infer ratio from input image for perfect overlay
            }
        });

        const candidate = response.candidates?.[0];
        const generatedPart = candidate?.content?.parts?.find(p => p.inlineData);

        // Fix TypeScript Error: Ensure we return string, not undefined
        if (!generatedPart?.inlineData?.data) {
            throw new Error("No image generated by Gemini or missing inlineData");
        }

        return {
            generatedImage: generatedPart.inlineData.data
        };
    }
);
